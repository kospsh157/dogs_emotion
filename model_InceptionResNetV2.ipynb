{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "F8leldBy6MKx"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-11-08 16:52:30.050494: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2023-11-08 16:52:30.350336: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2023-11-08 16:52:30.352259: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-11-08 16:52:31.498918: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import InceptionResNetV2\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "aT4qPHKN6gdz"
      },
      "outputs": [],
      "source": [
        "# 데이터셋 경로 설정\n",
        "base_dir = './DogEmotion'\n",
        "emotions = ['happy', 'sad', 'relaxed', 'angry']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_Bcegh76lg1",
        "outputId": "e3737f5d-eea0-4a4d-9d12-02e08ffc527b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_resnet_v2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "219055592/219055592 [==============================] - 22s 0us/step\n",
            "Index: 0, Layer Name: input_1\n",
            "Index: 1, Layer Name: conv2d\n",
            "Index: 2, Layer Name: batch_normalization\n",
            "Index: 3, Layer Name: activation\n",
            "Index: 4, Layer Name: conv2d_1\n",
            "Index: 5, Layer Name: batch_normalization_1\n",
            "Index: 6, Layer Name: activation_1\n",
            "Index: 7, Layer Name: conv2d_2\n",
            "Index: 8, Layer Name: batch_normalization_2\n",
            "Index: 9, Layer Name: activation_2\n",
            "Index: 10, Layer Name: max_pooling2d\n",
            "Index: 11, Layer Name: conv2d_3\n",
            "Index: 12, Layer Name: batch_normalization_3\n",
            "Index: 13, Layer Name: activation_3\n",
            "Index: 14, Layer Name: conv2d_4\n",
            "Index: 15, Layer Name: batch_normalization_4\n",
            "Index: 16, Layer Name: activation_4\n",
            "Index: 17, Layer Name: max_pooling2d_1\n",
            "Index: 18, Layer Name: conv2d_8\n",
            "Index: 19, Layer Name: batch_normalization_8\n",
            "Index: 20, Layer Name: activation_8\n",
            "Index: 21, Layer Name: conv2d_6\n",
            "Index: 22, Layer Name: conv2d_9\n",
            "Index: 23, Layer Name: batch_normalization_6\n",
            "Index: 24, Layer Name: batch_normalization_9\n",
            "Index: 25, Layer Name: activation_6\n",
            "Index: 26, Layer Name: activation_9\n",
            "Index: 27, Layer Name: average_pooling2d\n",
            "Index: 28, Layer Name: conv2d_5\n",
            "Index: 29, Layer Name: conv2d_7\n",
            "Index: 30, Layer Name: conv2d_10\n",
            "Index: 31, Layer Name: conv2d_11\n",
            "Index: 32, Layer Name: batch_normalization_5\n",
            "Index: 33, Layer Name: batch_normalization_7\n",
            "Index: 34, Layer Name: batch_normalization_10\n",
            "Index: 35, Layer Name: batch_normalization_11\n",
            "Index: 36, Layer Name: activation_5\n",
            "Index: 37, Layer Name: activation_7\n",
            "Index: 38, Layer Name: activation_10\n",
            "Index: 39, Layer Name: activation_11\n",
            "Index: 40, Layer Name: mixed_5b\n",
            "Index: 41, Layer Name: conv2d_15\n",
            "Index: 42, Layer Name: batch_normalization_15\n",
            "Index: 43, Layer Name: activation_15\n",
            "Index: 44, Layer Name: conv2d_13\n",
            "Index: 45, Layer Name: conv2d_16\n",
            "Index: 46, Layer Name: batch_normalization_13\n",
            "Index: 47, Layer Name: batch_normalization_16\n",
            "Index: 48, Layer Name: activation_13\n",
            "Index: 49, Layer Name: activation_16\n",
            "Index: 50, Layer Name: conv2d_12\n",
            "Index: 51, Layer Name: conv2d_14\n",
            "Index: 52, Layer Name: conv2d_17\n",
            "Index: 53, Layer Name: batch_normalization_12\n",
            "Index: 54, Layer Name: batch_normalization_14\n",
            "Index: 55, Layer Name: batch_normalization_17\n",
            "Index: 56, Layer Name: activation_12\n",
            "Index: 57, Layer Name: activation_14\n",
            "Index: 58, Layer Name: activation_17\n",
            "Index: 59, Layer Name: block35_1_mixed\n",
            "Index: 60, Layer Name: block35_1_conv\n",
            "Index: 61, Layer Name: custom_scale_layer\n",
            "Index: 62, Layer Name: block35_1_ac\n",
            "Index: 63, Layer Name: conv2d_21\n",
            "Index: 64, Layer Name: batch_normalization_21\n",
            "Index: 65, Layer Name: activation_21\n",
            "Index: 66, Layer Name: conv2d_19\n",
            "Index: 67, Layer Name: conv2d_22\n",
            "Index: 68, Layer Name: batch_normalization_19\n",
            "Index: 69, Layer Name: batch_normalization_22\n",
            "Index: 70, Layer Name: activation_19\n",
            "Index: 71, Layer Name: activation_22\n",
            "Index: 72, Layer Name: conv2d_18\n",
            "Index: 73, Layer Name: conv2d_20\n",
            "Index: 74, Layer Name: conv2d_23\n",
            "Index: 75, Layer Name: batch_normalization_18\n",
            "Index: 76, Layer Name: batch_normalization_20\n",
            "Index: 77, Layer Name: batch_normalization_23\n",
            "Index: 78, Layer Name: activation_18\n",
            "Index: 79, Layer Name: activation_20\n",
            "Index: 80, Layer Name: activation_23\n",
            "Index: 81, Layer Name: block35_2_mixed\n",
            "Index: 82, Layer Name: block35_2_conv\n",
            "Index: 83, Layer Name: custom_scale_layer_1\n",
            "Index: 84, Layer Name: block35_2_ac\n",
            "Index: 85, Layer Name: conv2d_27\n",
            "Index: 86, Layer Name: batch_normalization_27\n",
            "Index: 87, Layer Name: activation_27\n",
            "Index: 88, Layer Name: conv2d_25\n",
            "Index: 89, Layer Name: conv2d_28\n",
            "Index: 90, Layer Name: batch_normalization_25\n",
            "Index: 91, Layer Name: batch_normalization_28\n",
            "Index: 92, Layer Name: activation_25\n",
            "Index: 93, Layer Name: activation_28\n",
            "Index: 94, Layer Name: conv2d_24\n",
            "Index: 95, Layer Name: conv2d_26\n",
            "Index: 96, Layer Name: conv2d_29\n",
            "Index: 97, Layer Name: batch_normalization_24\n",
            "Index: 98, Layer Name: batch_normalization_26\n",
            "Index: 99, Layer Name: batch_normalization_29\n",
            "Index: 100, Layer Name: activation_24\n",
            "Index: 101, Layer Name: activation_26\n",
            "Index: 102, Layer Name: activation_29\n",
            "Index: 103, Layer Name: block35_3_mixed\n",
            "Index: 104, Layer Name: block35_3_conv\n",
            "Index: 105, Layer Name: custom_scale_layer_2\n",
            "Index: 106, Layer Name: block35_3_ac\n",
            "Index: 107, Layer Name: conv2d_33\n",
            "Index: 108, Layer Name: batch_normalization_33\n",
            "Index: 109, Layer Name: activation_33\n",
            "Index: 110, Layer Name: conv2d_31\n",
            "Index: 111, Layer Name: conv2d_34\n",
            "Index: 112, Layer Name: batch_normalization_31\n",
            "Index: 113, Layer Name: batch_normalization_34\n",
            "Index: 114, Layer Name: activation_31\n",
            "Index: 115, Layer Name: activation_34\n",
            "Index: 116, Layer Name: conv2d_30\n",
            "Index: 117, Layer Name: conv2d_32\n",
            "Index: 118, Layer Name: conv2d_35\n",
            "Index: 119, Layer Name: batch_normalization_30\n",
            "Index: 120, Layer Name: batch_normalization_32\n",
            "Index: 121, Layer Name: batch_normalization_35\n",
            "Index: 122, Layer Name: activation_30\n",
            "Index: 123, Layer Name: activation_32\n",
            "Index: 124, Layer Name: activation_35\n",
            "Index: 125, Layer Name: block35_4_mixed\n",
            "Index: 126, Layer Name: block35_4_conv\n",
            "Index: 127, Layer Name: custom_scale_layer_3\n",
            "Index: 128, Layer Name: block35_4_ac\n",
            "Index: 129, Layer Name: conv2d_39\n",
            "Index: 130, Layer Name: batch_normalization_39\n",
            "Index: 131, Layer Name: activation_39\n",
            "Index: 132, Layer Name: conv2d_37\n",
            "Index: 133, Layer Name: conv2d_40\n",
            "Index: 134, Layer Name: batch_normalization_37\n",
            "Index: 135, Layer Name: batch_normalization_40\n",
            "Index: 136, Layer Name: activation_37\n",
            "Index: 137, Layer Name: activation_40\n",
            "Index: 138, Layer Name: conv2d_36\n",
            "Index: 139, Layer Name: conv2d_38\n",
            "Index: 140, Layer Name: conv2d_41\n",
            "Index: 141, Layer Name: batch_normalization_36\n",
            "Index: 142, Layer Name: batch_normalization_38\n",
            "Index: 143, Layer Name: batch_normalization_41\n",
            "Index: 144, Layer Name: activation_36\n",
            "Index: 145, Layer Name: activation_38\n",
            "Index: 146, Layer Name: activation_41\n",
            "Index: 147, Layer Name: block35_5_mixed\n",
            "Index: 148, Layer Name: block35_5_conv\n",
            "Index: 149, Layer Name: custom_scale_layer_4\n",
            "Index: 150, Layer Name: block35_5_ac\n",
            "Index: 151, Layer Name: conv2d_45\n",
            "Index: 152, Layer Name: batch_normalization_45\n",
            "Index: 153, Layer Name: activation_45\n",
            "Index: 154, Layer Name: conv2d_43\n",
            "Index: 155, Layer Name: conv2d_46\n",
            "Index: 156, Layer Name: batch_normalization_43\n",
            "Index: 157, Layer Name: batch_normalization_46\n",
            "Index: 158, Layer Name: activation_43\n",
            "Index: 159, Layer Name: activation_46\n",
            "Index: 160, Layer Name: conv2d_42\n",
            "Index: 161, Layer Name: conv2d_44\n",
            "Index: 162, Layer Name: conv2d_47\n",
            "Index: 163, Layer Name: batch_normalization_42\n",
            "Index: 164, Layer Name: batch_normalization_44\n",
            "Index: 165, Layer Name: batch_normalization_47\n",
            "Index: 166, Layer Name: activation_42\n",
            "Index: 167, Layer Name: activation_44\n",
            "Index: 168, Layer Name: activation_47\n",
            "Index: 169, Layer Name: block35_6_mixed\n",
            "Index: 170, Layer Name: block35_6_conv\n",
            "Index: 171, Layer Name: custom_scale_layer_5\n",
            "Index: 172, Layer Name: block35_6_ac\n",
            "Index: 173, Layer Name: conv2d_51\n",
            "Index: 174, Layer Name: batch_normalization_51\n",
            "Index: 175, Layer Name: activation_51\n",
            "Index: 176, Layer Name: conv2d_49\n",
            "Index: 177, Layer Name: conv2d_52\n",
            "Index: 178, Layer Name: batch_normalization_49\n",
            "Index: 179, Layer Name: batch_normalization_52\n",
            "Index: 180, Layer Name: activation_49\n",
            "Index: 181, Layer Name: activation_52\n",
            "Index: 182, Layer Name: conv2d_48\n",
            "Index: 183, Layer Name: conv2d_50\n",
            "Index: 184, Layer Name: conv2d_53\n",
            "Index: 185, Layer Name: batch_normalization_48\n",
            "Index: 186, Layer Name: batch_normalization_50\n",
            "Index: 187, Layer Name: batch_normalization_53\n",
            "Index: 188, Layer Name: activation_48\n",
            "Index: 189, Layer Name: activation_50\n",
            "Index: 190, Layer Name: activation_53\n",
            "Index: 191, Layer Name: block35_7_mixed\n",
            "Index: 192, Layer Name: block35_7_conv\n",
            "Index: 193, Layer Name: custom_scale_layer_6\n",
            "Index: 194, Layer Name: block35_7_ac\n",
            "Index: 195, Layer Name: conv2d_57\n",
            "Index: 196, Layer Name: batch_normalization_57\n",
            "Index: 197, Layer Name: activation_57\n",
            "Index: 198, Layer Name: conv2d_55\n",
            "Index: 199, Layer Name: conv2d_58\n",
            "Index: 200, Layer Name: batch_normalization_55\n",
            "Index: 201, Layer Name: batch_normalization_58\n",
            "Index: 202, Layer Name: activation_55\n",
            "Index: 203, Layer Name: activation_58\n",
            "Index: 204, Layer Name: conv2d_54\n",
            "Index: 205, Layer Name: conv2d_56\n",
            "Index: 206, Layer Name: conv2d_59\n",
            "Index: 207, Layer Name: batch_normalization_54\n",
            "Index: 208, Layer Name: batch_normalization_56\n",
            "Index: 209, Layer Name: batch_normalization_59\n",
            "Index: 210, Layer Name: activation_54\n",
            "Index: 211, Layer Name: activation_56\n",
            "Index: 212, Layer Name: activation_59\n",
            "Index: 213, Layer Name: block35_8_mixed\n",
            "Index: 214, Layer Name: block35_8_conv\n",
            "Index: 215, Layer Name: custom_scale_layer_7\n",
            "Index: 216, Layer Name: block35_8_ac\n",
            "Index: 217, Layer Name: conv2d_63\n",
            "Index: 218, Layer Name: batch_normalization_63\n",
            "Index: 219, Layer Name: activation_63\n",
            "Index: 220, Layer Name: conv2d_61\n",
            "Index: 221, Layer Name: conv2d_64\n",
            "Index: 222, Layer Name: batch_normalization_61\n",
            "Index: 223, Layer Name: batch_normalization_64\n",
            "Index: 224, Layer Name: activation_61\n",
            "Index: 225, Layer Name: activation_64\n",
            "Index: 226, Layer Name: conv2d_60\n",
            "Index: 227, Layer Name: conv2d_62\n",
            "Index: 228, Layer Name: conv2d_65\n",
            "Index: 229, Layer Name: batch_normalization_60\n",
            "Index: 230, Layer Name: batch_normalization_62\n",
            "Index: 231, Layer Name: batch_normalization_65\n",
            "Index: 232, Layer Name: activation_60\n",
            "Index: 233, Layer Name: activation_62\n",
            "Index: 234, Layer Name: activation_65\n",
            "Index: 235, Layer Name: block35_9_mixed\n",
            "Index: 236, Layer Name: block35_9_conv\n",
            "Index: 237, Layer Name: custom_scale_layer_8\n",
            "Index: 238, Layer Name: block35_9_ac\n",
            "Index: 239, Layer Name: conv2d_69\n",
            "Index: 240, Layer Name: batch_normalization_69\n",
            "Index: 241, Layer Name: activation_69\n",
            "Index: 242, Layer Name: conv2d_67\n",
            "Index: 243, Layer Name: conv2d_70\n",
            "Index: 244, Layer Name: batch_normalization_67\n",
            "Index: 245, Layer Name: batch_normalization_70\n",
            "Index: 246, Layer Name: activation_67\n",
            "Index: 247, Layer Name: activation_70\n",
            "Index: 248, Layer Name: conv2d_66\n",
            "Index: 249, Layer Name: conv2d_68\n",
            "Index: 250, Layer Name: conv2d_71\n",
            "Index: 251, Layer Name: batch_normalization_66\n",
            "Index: 252, Layer Name: batch_normalization_68\n",
            "Index: 253, Layer Name: batch_normalization_71\n",
            "Index: 254, Layer Name: activation_66\n",
            "Index: 255, Layer Name: activation_68\n",
            "Index: 256, Layer Name: activation_71\n",
            "Index: 257, Layer Name: block35_10_mixed\n",
            "Index: 258, Layer Name: block35_10_conv\n",
            "Index: 259, Layer Name: custom_scale_layer_9\n",
            "Index: 260, Layer Name: block35_10_ac\n",
            "Index: 261, Layer Name: conv2d_73\n",
            "Index: 262, Layer Name: batch_normalization_73\n",
            "Index: 263, Layer Name: activation_73\n",
            "Index: 264, Layer Name: conv2d_74\n",
            "Index: 265, Layer Name: batch_normalization_74\n",
            "Index: 266, Layer Name: activation_74\n",
            "Index: 267, Layer Name: conv2d_72\n",
            "Index: 268, Layer Name: conv2d_75\n",
            "Index: 269, Layer Name: batch_normalization_72\n",
            "Index: 270, Layer Name: batch_normalization_75\n",
            "Index: 271, Layer Name: activation_72\n",
            "Index: 272, Layer Name: activation_75\n",
            "Index: 273, Layer Name: max_pooling2d_2\n",
            "Index: 274, Layer Name: mixed_6a\n",
            "Index: 275, Layer Name: conv2d_77\n",
            "Index: 276, Layer Name: batch_normalization_77\n",
            "Index: 277, Layer Name: activation_77\n",
            "Index: 278, Layer Name: conv2d_78\n",
            "Index: 279, Layer Name: batch_normalization_78\n",
            "Index: 280, Layer Name: activation_78\n",
            "Index: 281, Layer Name: conv2d_76\n",
            "Index: 282, Layer Name: conv2d_79\n",
            "Index: 283, Layer Name: batch_normalization_76\n",
            "Index: 284, Layer Name: batch_normalization_79\n",
            "Index: 285, Layer Name: activation_76\n",
            "Index: 286, Layer Name: activation_79\n",
            "Index: 287, Layer Name: block17_1_mixed\n",
            "Index: 288, Layer Name: block17_1_conv\n",
            "Index: 289, Layer Name: custom_scale_layer_10\n",
            "Index: 290, Layer Name: block17_1_ac\n",
            "Index: 291, Layer Name: conv2d_81\n",
            "Index: 292, Layer Name: batch_normalization_81\n",
            "Index: 293, Layer Name: activation_81\n",
            "Index: 294, Layer Name: conv2d_82\n",
            "Index: 295, Layer Name: batch_normalization_82\n",
            "Index: 296, Layer Name: activation_82\n",
            "Index: 297, Layer Name: conv2d_80\n",
            "Index: 298, Layer Name: conv2d_83\n",
            "Index: 299, Layer Name: batch_normalization_80\n",
            "Index: 300, Layer Name: batch_normalization_83\n",
            "Index: 301, Layer Name: activation_80\n",
            "Index: 302, Layer Name: activation_83\n",
            "Index: 303, Layer Name: block17_2_mixed\n",
            "Index: 304, Layer Name: block17_2_conv\n",
            "Index: 305, Layer Name: custom_scale_layer_11\n",
            "Index: 306, Layer Name: block17_2_ac\n",
            "Index: 307, Layer Name: conv2d_85\n",
            "Index: 308, Layer Name: batch_normalization_85\n",
            "Index: 309, Layer Name: activation_85\n",
            "Index: 310, Layer Name: conv2d_86\n",
            "Index: 311, Layer Name: batch_normalization_86\n",
            "Index: 312, Layer Name: activation_86\n",
            "Index: 313, Layer Name: conv2d_84\n",
            "Index: 314, Layer Name: conv2d_87\n",
            "Index: 315, Layer Name: batch_normalization_84\n",
            "Index: 316, Layer Name: batch_normalization_87\n",
            "Index: 317, Layer Name: activation_84\n",
            "Index: 318, Layer Name: activation_87\n",
            "Index: 319, Layer Name: block17_3_mixed\n",
            "Index: 320, Layer Name: block17_3_conv\n",
            "Index: 321, Layer Name: custom_scale_layer_12\n",
            "Index: 322, Layer Name: block17_3_ac\n",
            "Index: 323, Layer Name: conv2d_89\n",
            "Index: 324, Layer Name: batch_normalization_89\n",
            "Index: 325, Layer Name: activation_89\n",
            "Index: 326, Layer Name: conv2d_90\n",
            "Index: 327, Layer Name: batch_normalization_90\n",
            "Index: 328, Layer Name: activation_90\n",
            "Index: 329, Layer Name: conv2d_88\n",
            "Index: 330, Layer Name: conv2d_91\n",
            "Index: 331, Layer Name: batch_normalization_88\n",
            "Index: 332, Layer Name: batch_normalization_91\n",
            "Index: 333, Layer Name: activation_88\n",
            "Index: 334, Layer Name: activation_91\n",
            "Index: 335, Layer Name: block17_4_mixed\n",
            "Index: 336, Layer Name: block17_4_conv\n",
            "Index: 337, Layer Name: custom_scale_layer_13\n",
            "Index: 338, Layer Name: block17_4_ac\n",
            "Index: 339, Layer Name: conv2d_93\n",
            "Index: 340, Layer Name: batch_normalization_93\n",
            "Index: 341, Layer Name: activation_93\n",
            "Index: 342, Layer Name: conv2d_94\n",
            "Index: 343, Layer Name: batch_normalization_94\n",
            "Index: 344, Layer Name: activation_94\n",
            "Index: 345, Layer Name: conv2d_92\n",
            "Index: 346, Layer Name: conv2d_95\n",
            "Index: 347, Layer Name: batch_normalization_92\n",
            "Index: 348, Layer Name: batch_normalization_95\n",
            "Index: 349, Layer Name: activation_92\n",
            "Index: 350, Layer Name: activation_95\n",
            "Index: 351, Layer Name: block17_5_mixed\n",
            "Index: 352, Layer Name: block17_5_conv\n",
            "Index: 353, Layer Name: custom_scale_layer_14\n",
            "Index: 354, Layer Name: block17_5_ac\n",
            "Index: 355, Layer Name: conv2d_97\n",
            "Index: 356, Layer Name: batch_normalization_97\n",
            "Index: 357, Layer Name: activation_97\n",
            "Index: 358, Layer Name: conv2d_98\n",
            "Index: 359, Layer Name: batch_normalization_98\n",
            "Index: 360, Layer Name: activation_98\n",
            "Index: 361, Layer Name: conv2d_96\n",
            "Index: 362, Layer Name: conv2d_99\n",
            "Index: 363, Layer Name: batch_normalization_96\n",
            "Index: 364, Layer Name: batch_normalization_99\n",
            "Index: 365, Layer Name: activation_96\n",
            "Index: 366, Layer Name: activation_99\n",
            "Index: 367, Layer Name: block17_6_mixed\n",
            "Index: 368, Layer Name: block17_6_conv\n",
            "Index: 369, Layer Name: custom_scale_layer_15\n",
            "Index: 370, Layer Name: block17_6_ac\n",
            "Index: 371, Layer Name: conv2d_101\n",
            "Index: 372, Layer Name: batch_normalization_101\n",
            "Index: 373, Layer Name: activation_101\n",
            "Index: 374, Layer Name: conv2d_102\n",
            "Index: 375, Layer Name: batch_normalization_102\n",
            "Index: 376, Layer Name: activation_102\n",
            "Index: 377, Layer Name: conv2d_100\n",
            "Index: 378, Layer Name: conv2d_103\n",
            "Index: 379, Layer Name: batch_normalization_100\n",
            "Index: 380, Layer Name: batch_normalization_103\n",
            "Index: 381, Layer Name: activation_100\n",
            "Index: 382, Layer Name: activation_103\n",
            "Index: 383, Layer Name: block17_7_mixed\n",
            "Index: 384, Layer Name: block17_7_conv\n",
            "Index: 385, Layer Name: custom_scale_layer_16\n",
            "Index: 386, Layer Name: block17_7_ac\n",
            "Index: 387, Layer Name: conv2d_105\n",
            "Index: 388, Layer Name: batch_normalization_105\n",
            "Index: 389, Layer Name: activation_105\n",
            "Index: 390, Layer Name: conv2d_106\n",
            "Index: 391, Layer Name: batch_normalization_106\n",
            "Index: 392, Layer Name: activation_106\n",
            "Index: 393, Layer Name: conv2d_104\n",
            "Index: 394, Layer Name: conv2d_107\n",
            "Index: 395, Layer Name: batch_normalization_104\n",
            "Index: 396, Layer Name: batch_normalization_107\n",
            "Index: 397, Layer Name: activation_104\n",
            "Index: 398, Layer Name: activation_107\n",
            "Index: 399, Layer Name: block17_8_mixed\n",
            "Index: 400, Layer Name: block17_8_conv\n",
            "Index: 401, Layer Name: custom_scale_layer_17\n",
            "Index: 402, Layer Name: block17_8_ac\n",
            "Index: 403, Layer Name: conv2d_109\n",
            "Index: 404, Layer Name: batch_normalization_109\n",
            "Index: 405, Layer Name: activation_109\n",
            "Index: 406, Layer Name: conv2d_110\n",
            "Index: 407, Layer Name: batch_normalization_110\n",
            "Index: 408, Layer Name: activation_110\n",
            "Index: 409, Layer Name: conv2d_108\n",
            "Index: 410, Layer Name: conv2d_111\n",
            "Index: 411, Layer Name: batch_normalization_108\n",
            "Index: 412, Layer Name: batch_normalization_111\n",
            "Index: 413, Layer Name: activation_108\n",
            "Index: 414, Layer Name: activation_111\n",
            "Index: 415, Layer Name: block17_9_mixed\n",
            "Index: 416, Layer Name: block17_9_conv\n",
            "Index: 417, Layer Name: custom_scale_layer_18\n",
            "Index: 418, Layer Name: block17_9_ac\n",
            "Index: 419, Layer Name: conv2d_113\n",
            "Index: 420, Layer Name: batch_normalization_113\n",
            "Index: 421, Layer Name: activation_113\n",
            "Index: 422, Layer Name: conv2d_114\n",
            "Index: 423, Layer Name: batch_normalization_114\n",
            "Index: 424, Layer Name: activation_114\n",
            "Index: 425, Layer Name: conv2d_112\n",
            "Index: 426, Layer Name: conv2d_115\n",
            "Index: 427, Layer Name: batch_normalization_112\n",
            "Index: 428, Layer Name: batch_normalization_115\n",
            "Index: 429, Layer Name: activation_112\n",
            "Index: 430, Layer Name: activation_115\n",
            "Index: 431, Layer Name: block17_10_mixed\n",
            "Index: 432, Layer Name: block17_10_conv\n",
            "Index: 433, Layer Name: custom_scale_layer_19\n",
            "Index: 434, Layer Name: block17_10_ac\n",
            "Index: 435, Layer Name: conv2d_117\n",
            "Index: 436, Layer Name: batch_normalization_117\n",
            "Index: 437, Layer Name: activation_117\n",
            "Index: 438, Layer Name: conv2d_118\n",
            "Index: 439, Layer Name: batch_normalization_118\n",
            "Index: 440, Layer Name: activation_118\n",
            "Index: 441, Layer Name: conv2d_116\n",
            "Index: 442, Layer Name: conv2d_119\n",
            "Index: 443, Layer Name: batch_normalization_116\n",
            "Index: 444, Layer Name: batch_normalization_119\n",
            "Index: 445, Layer Name: activation_116\n",
            "Index: 446, Layer Name: activation_119\n",
            "Index: 447, Layer Name: block17_11_mixed\n",
            "Index: 448, Layer Name: block17_11_conv\n",
            "Index: 449, Layer Name: custom_scale_layer_20\n",
            "Index: 450, Layer Name: block17_11_ac\n",
            "Index: 451, Layer Name: conv2d_121\n",
            "Index: 452, Layer Name: batch_normalization_121\n",
            "Index: 453, Layer Name: activation_121\n",
            "Index: 454, Layer Name: conv2d_122\n",
            "Index: 455, Layer Name: batch_normalization_122\n",
            "Index: 456, Layer Name: activation_122\n",
            "Index: 457, Layer Name: conv2d_120\n",
            "Index: 458, Layer Name: conv2d_123\n",
            "Index: 459, Layer Name: batch_normalization_120\n",
            "Index: 460, Layer Name: batch_normalization_123\n",
            "Index: 461, Layer Name: activation_120\n",
            "Index: 462, Layer Name: activation_123\n",
            "Index: 463, Layer Name: block17_12_mixed\n",
            "Index: 464, Layer Name: block17_12_conv\n",
            "Index: 465, Layer Name: custom_scale_layer_21\n",
            "Index: 466, Layer Name: block17_12_ac\n",
            "Index: 467, Layer Name: conv2d_125\n",
            "Index: 468, Layer Name: batch_normalization_125\n",
            "Index: 469, Layer Name: activation_125\n",
            "Index: 470, Layer Name: conv2d_126\n",
            "Index: 471, Layer Name: batch_normalization_126\n",
            "Index: 472, Layer Name: activation_126\n",
            "Index: 473, Layer Name: conv2d_124\n",
            "Index: 474, Layer Name: conv2d_127\n",
            "Index: 475, Layer Name: batch_normalization_124\n",
            "Index: 476, Layer Name: batch_normalization_127\n",
            "Index: 477, Layer Name: activation_124\n",
            "Index: 478, Layer Name: activation_127\n",
            "Index: 479, Layer Name: block17_13_mixed\n",
            "Index: 480, Layer Name: block17_13_conv\n",
            "Index: 481, Layer Name: custom_scale_layer_22\n",
            "Index: 482, Layer Name: block17_13_ac\n",
            "Index: 483, Layer Name: conv2d_129\n",
            "Index: 484, Layer Name: batch_normalization_129\n",
            "Index: 485, Layer Name: activation_129\n",
            "Index: 486, Layer Name: conv2d_130\n",
            "Index: 487, Layer Name: batch_normalization_130\n",
            "Index: 488, Layer Name: activation_130\n",
            "Index: 489, Layer Name: conv2d_128\n",
            "Index: 490, Layer Name: conv2d_131\n",
            "Index: 491, Layer Name: batch_normalization_128\n",
            "Index: 492, Layer Name: batch_normalization_131\n",
            "Index: 493, Layer Name: activation_128\n",
            "Index: 494, Layer Name: activation_131\n",
            "Index: 495, Layer Name: block17_14_mixed\n",
            "Index: 496, Layer Name: block17_14_conv\n",
            "Index: 497, Layer Name: custom_scale_layer_23\n",
            "Index: 498, Layer Name: block17_14_ac\n",
            "Index: 499, Layer Name: conv2d_133\n",
            "Index: 500, Layer Name: batch_normalization_133\n",
            "Index: 501, Layer Name: activation_133\n",
            "Index: 502, Layer Name: conv2d_134\n",
            "Index: 503, Layer Name: batch_normalization_134\n",
            "Index: 504, Layer Name: activation_134\n",
            "Index: 505, Layer Name: conv2d_132\n",
            "Index: 506, Layer Name: conv2d_135\n",
            "Index: 507, Layer Name: batch_normalization_132\n",
            "Index: 508, Layer Name: batch_normalization_135\n",
            "Index: 509, Layer Name: activation_132\n",
            "Index: 510, Layer Name: activation_135\n",
            "Index: 511, Layer Name: block17_15_mixed\n",
            "Index: 512, Layer Name: block17_15_conv\n",
            "Index: 513, Layer Name: custom_scale_layer_24\n",
            "Index: 514, Layer Name: block17_15_ac\n",
            "Index: 515, Layer Name: conv2d_137\n",
            "Index: 516, Layer Name: batch_normalization_137\n",
            "Index: 517, Layer Name: activation_137\n",
            "Index: 518, Layer Name: conv2d_138\n",
            "Index: 519, Layer Name: batch_normalization_138\n",
            "Index: 520, Layer Name: activation_138\n",
            "Index: 521, Layer Name: conv2d_136\n",
            "Index: 522, Layer Name: conv2d_139\n",
            "Index: 523, Layer Name: batch_normalization_136\n",
            "Index: 524, Layer Name: batch_normalization_139\n",
            "Index: 525, Layer Name: activation_136\n",
            "Index: 526, Layer Name: activation_139\n",
            "Index: 527, Layer Name: block17_16_mixed\n",
            "Index: 528, Layer Name: block17_16_conv\n",
            "Index: 529, Layer Name: custom_scale_layer_25\n",
            "Index: 530, Layer Name: block17_16_ac\n",
            "Index: 531, Layer Name: conv2d_141\n",
            "Index: 532, Layer Name: batch_normalization_141\n",
            "Index: 533, Layer Name: activation_141\n",
            "Index: 534, Layer Name: conv2d_142\n",
            "Index: 535, Layer Name: batch_normalization_142\n",
            "Index: 536, Layer Name: activation_142\n",
            "Index: 537, Layer Name: conv2d_140\n",
            "Index: 538, Layer Name: conv2d_143\n",
            "Index: 539, Layer Name: batch_normalization_140\n",
            "Index: 540, Layer Name: batch_normalization_143\n",
            "Index: 541, Layer Name: activation_140\n",
            "Index: 542, Layer Name: activation_143\n",
            "Index: 543, Layer Name: block17_17_mixed\n",
            "Index: 544, Layer Name: block17_17_conv\n",
            "Index: 545, Layer Name: custom_scale_layer_26\n",
            "Index: 546, Layer Name: block17_17_ac\n",
            "Index: 547, Layer Name: conv2d_145\n",
            "Index: 548, Layer Name: batch_normalization_145\n",
            "Index: 549, Layer Name: activation_145\n",
            "Index: 550, Layer Name: conv2d_146\n",
            "Index: 551, Layer Name: batch_normalization_146\n",
            "Index: 552, Layer Name: activation_146\n",
            "Index: 553, Layer Name: conv2d_144\n",
            "Index: 554, Layer Name: conv2d_147\n",
            "Index: 555, Layer Name: batch_normalization_144\n",
            "Index: 556, Layer Name: batch_normalization_147\n",
            "Index: 557, Layer Name: activation_144\n",
            "Index: 558, Layer Name: activation_147\n",
            "Index: 559, Layer Name: block17_18_mixed\n",
            "Index: 560, Layer Name: block17_18_conv\n",
            "Index: 561, Layer Name: custom_scale_layer_27\n",
            "Index: 562, Layer Name: block17_18_ac\n",
            "Index: 563, Layer Name: conv2d_149\n",
            "Index: 564, Layer Name: batch_normalization_149\n",
            "Index: 565, Layer Name: activation_149\n",
            "Index: 566, Layer Name: conv2d_150\n",
            "Index: 567, Layer Name: batch_normalization_150\n",
            "Index: 568, Layer Name: activation_150\n",
            "Index: 569, Layer Name: conv2d_148\n",
            "Index: 570, Layer Name: conv2d_151\n",
            "Index: 571, Layer Name: batch_normalization_148\n",
            "Index: 572, Layer Name: batch_normalization_151\n",
            "Index: 573, Layer Name: activation_148\n",
            "Index: 574, Layer Name: activation_151\n",
            "Index: 575, Layer Name: block17_19_mixed\n",
            "Index: 576, Layer Name: block17_19_conv\n",
            "Index: 577, Layer Name: custom_scale_layer_28\n",
            "Index: 578, Layer Name: block17_19_ac\n",
            "Index: 579, Layer Name: conv2d_153\n",
            "Index: 580, Layer Name: batch_normalization_153\n",
            "Index: 581, Layer Name: activation_153\n",
            "Index: 582, Layer Name: conv2d_154\n",
            "Index: 583, Layer Name: batch_normalization_154\n",
            "Index: 584, Layer Name: activation_154\n",
            "Index: 585, Layer Name: conv2d_152\n",
            "Index: 586, Layer Name: conv2d_155\n",
            "Index: 587, Layer Name: batch_normalization_152\n",
            "Index: 588, Layer Name: batch_normalization_155\n",
            "Index: 589, Layer Name: activation_152\n",
            "Index: 590, Layer Name: activation_155\n",
            "Index: 591, Layer Name: block17_20_mixed\n",
            "Index: 592, Layer Name: block17_20_conv\n",
            "Index: 593, Layer Name: custom_scale_layer_29\n",
            "Index: 594, Layer Name: block17_20_ac\n",
            "Index: 595, Layer Name: conv2d_160\n",
            "Index: 596, Layer Name: batch_normalization_160\n",
            "Index: 597, Layer Name: activation_160\n",
            "Index: 598, Layer Name: conv2d_156\n",
            "Index: 599, Layer Name: conv2d_158\n",
            "Index: 600, Layer Name: conv2d_161\n",
            "Index: 601, Layer Name: batch_normalization_156\n",
            "Index: 602, Layer Name: batch_normalization_158\n",
            "Index: 603, Layer Name: batch_normalization_161\n",
            "Index: 604, Layer Name: activation_156\n",
            "Index: 605, Layer Name: activation_158\n",
            "Index: 606, Layer Name: activation_161\n",
            "Index: 607, Layer Name: conv2d_157\n",
            "Index: 608, Layer Name: conv2d_159\n",
            "Index: 609, Layer Name: conv2d_162\n",
            "Index: 610, Layer Name: batch_normalization_157\n",
            "Index: 611, Layer Name: batch_normalization_159\n",
            "Index: 612, Layer Name: batch_normalization_162\n",
            "Index: 613, Layer Name: activation_157\n",
            "Index: 614, Layer Name: activation_159\n",
            "Index: 615, Layer Name: activation_162\n",
            "Index: 616, Layer Name: max_pooling2d_3\n",
            "Index: 617, Layer Name: mixed_7a\n",
            "Index: 618, Layer Name: conv2d_164\n",
            "Index: 619, Layer Name: batch_normalization_164\n",
            "Index: 620, Layer Name: activation_164\n",
            "Index: 621, Layer Name: conv2d_165\n",
            "Index: 622, Layer Name: batch_normalization_165\n",
            "Index: 623, Layer Name: activation_165\n",
            "Index: 624, Layer Name: conv2d_163\n",
            "Index: 625, Layer Name: conv2d_166\n",
            "Index: 626, Layer Name: batch_normalization_163\n",
            "Index: 627, Layer Name: batch_normalization_166\n",
            "Index: 628, Layer Name: activation_163\n",
            "Index: 629, Layer Name: activation_166\n",
            "Index: 630, Layer Name: block8_1_mixed\n",
            "Index: 631, Layer Name: block8_1_conv\n",
            "Index: 632, Layer Name: custom_scale_layer_30\n",
            "Index: 633, Layer Name: block8_1_ac\n",
            "Index: 634, Layer Name: conv2d_168\n",
            "Index: 635, Layer Name: batch_normalization_168\n",
            "Index: 636, Layer Name: activation_168\n",
            "Index: 637, Layer Name: conv2d_169\n",
            "Index: 638, Layer Name: batch_normalization_169\n",
            "Index: 639, Layer Name: activation_169\n",
            "Index: 640, Layer Name: conv2d_167\n",
            "Index: 641, Layer Name: conv2d_170\n",
            "Index: 642, Layer Name: batch_normalization_167\n",
            "Index: 643, Layer Name: batch_normalization_170\n",
            "Index: 644, Layer Name: activation_167\n",
            "Index: 645, Layer Name: activation_170\n",
            "Index: 646, Layer Name: block8_2_mixed\n",
            "Index: 647, Layer Name: block8_2_conv\n",
            "Index: 648, Layer Name: custom_scale_layer_31\n",
            "Index: 649, Layer Name: block8_2_ac\n",
            "Index: 650, Layer Name: conv2d_172\n",
            "Index: 651, Layer Name: batch_normalization_172\n",
            "Index: 652, Layer Name: activation_172\n",
            "Index: 653, Layer Name: conv2d_173\n",
            "Index: 654, Layer Name: batch_normalization_173\n",
            "Index: 655, Layer Name: activation_173\n",
            "Index: 656, Layer Name: conv2d_171\n",
            "Index: 657, Layer Name: conv2d_174\n",
            "Index: 658, Layer Name: batch_normalization_171\n",
            "Index: 659, Layer Name: batch_normalization_174\n",
            "Index: 660, Layer Name: activation_171\n",
            "Index: 661, Layer Name: activation_174\n",
            "Index: 662, Layer Name: block8_3_mixed\n",
            "Index: 663, Layer Name: block8_3_conv\n",
            "Index: 664, Layer Name: custom_scale_layer_32\n",
            "Index: 665, Layer Name: block8_3_ac\n",
            "Index: 666, Layer Name: conv2d_176\n",
            "Index: 667, Layer Name: batch_normalization_176\n",
            "Index: 668, Layer Name: activation_176\n",
            "Index: 669, Layer Name: conv2d_177\n",
            "Index: 670, Layer Name: batch_normalization_177\n",
            "Index: 671, Layer Name: activation_177\n",
            "Index: 672, Layer Name: conv2d_175\n",
            "Index: 673, Layer Name: conv2d_178\n",
            "Index: 674, Layer Name: batch_normalization_175\n",
            "Index: 675, Layer Name: batch_normalization_178\n",
            "Index: 676, Layer Name: activation_175\n",
            "Index: 677, Layer Name: activation_178\n",
            "Index: 678, Layer Name: block8_4_mixed\n",
            "Index: 679, Layer Name: block8_4_conv\n",
            "Index: 680, Layer Name: custom_scale_layer_33\n",
            "Index: 681, Layer Name: block8_4_ac\n",
            "Index: 682, Layer Name: conv2d_180\n",
            "Index: 683, Layer Name: batch_normalization_180\n",
            "Index: 684, Layer Name: activation_180\n",
            "Index: 685, Layer Name: conv2d_181\n",
            "Index: 686, Layer Name: batch_normalization_181\n",
            "Index: 687, Layer Name: activation_181\n",
            "Index: 688, Layer Name: conv2d_179\n",
            "Index: 689, Layer Name: conv2d_182\n",
            "Index: 690, Layer Name: batch_normalization_179\n",
            "Index: 691, Layer Name: batch_normalization_182\n",
            "Index: 692, Layer Name: activation_179\n",
            "Index: 693, Layer Name: activation_182\n",
            "Index: 694, Layer Name: block8_5_mixed\n",
            "Index: 695, Layer Name: block8_5_conv\n",
            "Index: 696, Layer Name: custom_scale_layer_34\n",
            "Index: 697, Layer Name: block8_5_ac\n",
            "Index: 698, Layer Name: conv2d_184\n",
            "Index: 699, Layer Name: batch_normalization_184\n",
            "Index: 700, Layer Name: activation_184\n",
            "Index: 701, Layer Name: conv2d_185\n",
            "Index: 702, Layer Name: batch_normalization_185\n",
            "Index: 703, Layer Name: activation_185\n",
            "Index: 704, Layer Name: conv2d_183\n",
            "Index: 705, Layer Name: conv2d_186\n",
            "Index: 706, Layer Name: batch_normalization_183\n",
            "Index: 707, Layer Name: batch_normalization_186\n",
            "Index: 708, Layer Name: activation_183\n",
            "Index: 709, Layer Name: activation_186\n",
            "Index: 710, Layer Name: block8_6_mixed\n",
            "Index: 711, Layer Name: block8_6_conv\n",
            "Index: 712, Layer Name: custom_scale_layer_35\n",
            "Index: 713, Layer Name: block8_6_ac\n",
            "Index: 714, Layer Name: conv2d_188\n",
            "Index: 715, Layer Name: batch_normalization_188\n",
            "Index: 716, Layer Name: activation_188\n",
            "Index: 717, Layer Name: conv2d_189\n",
            "Index: 718, Layer Name: batch_normalization_189\n",
            "Index: 719, Layer Name: activation_189\n",
            "Index: 720, Layer Name: conv2d_187\n",
            "Index: 721, Layer Name: conv2d_190\n",
            "Index: 722, Layer Name: batch_normalization_187\n",
            "Index: 723, Layer Name: batch_normalization_190\n",
            "Index: 724, Layer Name: activation_187\n",
            "Index: 725, Layer Name: activation_190\n",
            "Index: 726, Layer Name: block8_7_mixed\n",
            "Index: 727, Layer Name: block8_7_conv\n",
            "Index: 728, Layer Name: custom_scale_layer_36\n",
            "Index: 729, Layer Name: block8_7_ac\n",
            "Index: 730, Layer Name: conv2d_192\n",
            "Index: 731, Layer Name: batch_normalization_192\n",
            "Index: 732, Layer Name: activation_192\n",
            "Index: 733, Layer Name: conv2d_193\n",
            "Index: 734, Layer Name: batch_normalization_193\n",
            "Index: 735, Layer Name: activation_193\n",
            "Index: 736, Layer Name: conv2d_191\n",
            "Index: 737, Layer Name: conv2d_194\n",
            "Index: 738, Layer Name: batch_normalization_191\n",
            "Index: 739, Layer Name: batch_normalization_194\n",
            "Index: 740, Layer Name: activation_191\n",
            "Index: 741, Layer Name: activation_194\n",
            "Index: 742, Layer Name: block8_8_mixed\n",
            "Index: 743, Layer Name: block8_8_conv\n",
            "Index: 744, Layer Name: custom_scale_layer_37\n",
            "Index: 745, Layer Name: block8_8_ac\n",
            "Index: 746, Layer Name: conv2d_196\n",
            "Index: 747, Layer Name: batch_normalization_196\n",
            "Index: 748, Layer Name: activation_196\n",
            "Index: 749, Layer Name: conv2d_197\n",
            "Index: 750, Layer Name: batch_normalization_197\n",
            "Index: 751, Layer Name: activation_197\n",
            "Index: 752, Layer Name: conv2d_195\n",
            "Index: 753, Layer Name: conv2d_198\n",
            "Index: 754, Layer Name: batch_normalization_195\n",
            "Index: 755, Layer Name: batch_normalization_198\n",
            "Index: 756, Layer Name: activation_195\n",
            "Index: 757, Layer Name: activation_198\n",
            "Index: 758, Layer Name: block8_9_mixed\n",
            "Index: 759, Layer Name: block8_9_conv\n",
            "Index: 760, Layer Name: custom_scale_layer_38\n",
            "Index: 761, Layer Name: block8_9_ac\n",
            "Index: 762, Layer Name: conv2d_200\n",
            "Index: 763, Layer Name: batch_normalization_200\n",
            "Index: 764, Layer Name: activation_200\n",
            "Index: 765, Layer Name: conv2d_201\n",
            "Index: 766, Layer Name: batch_normalization_201\n",
            "Index: 767, Layer Name: activation_201\n",
            "Index: 768, Layer Name: conv2d_199\n",
            "Index: 769, Layer Name: conv2d_202\n",
            "Index: 770, Layer Name: batch_normalization_199\n",
            "Index: 771, Layer Name: batch_normalization_202\n",
            "Index: 772, Layer Name: activation_199\n",
            "Index: 773, Layer Name: activation_202\n",
            "Index: 774, Layer Name: block8_10_mixed\n",
            "Index: 775, Layer Name: block8_10_conv\n",
            "Index: 776, Layer Name: custom_scale_layer_39\n",
            "Index: 777, Layer Name: conv_7b\n",
            "Index: 778, Layer Name: conv_7b_bn\n",
            "Index: 779, Layer Name: conv_7b_ac\n"
          ]
        }
      ],
      "source": [
        "# # Train, Validation 데이터셋 분리\n",
        "# train_datagen = ImageDataGenerator(\n",
        "#     validation_split=0.2,  # 20%를 검증 데이터로 사용\n",
        "#     rescale=1./255,\n",
        "#     rotation_range=20,\n",
        "#     width_shift_range=0.2,\n",
        "#     height_shift_range=0.2,\n",
        "#     shear_range=0.2,\n",
        "#     zoom_range=0.2,\n",
        "#     horizontal_flip=True,\n",
        "#     fill_mode='nearest'\n",
        "# )\n",
        "\n",
        "# train_generator = train_datagen.flow_from_directory(\n",
        "#     base_dir,\n",
        "#     target_size=(299, 299),\n",
        "#     batch_size=16,\n",
        "#     class_mode='categorical',\n",
        "#     subset='training'  # 학습 데이터 부분\n",
        "# )\n",
        "\n",
        "# validation_generator = train_datagen.flow_from_directory(\n",
        "#     base_dir,\n",
        "#     target_size=(299, 299),\n",
        "#     batch_size=16,\n",
        "#     class_mode='categorical',\n",
        "#     subset='validation'  # 검증 데이터 부분\n",
        "# )\n",
        "\n",
        "# InceptionResNetV2 모델\n",
        "base_model = InceptionResNetV2(weights='imagenet', include_top=False)\n",
        "\n",
        "# 새로운 출력 레이어 추가\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "predictions = Dense(4, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# 모델 구조 확인\n",
        "# model.summary()\n",
        "\n",
        "def print_layers_with_index(base_model):\n",
        "    for i, layer in enumerate(base_model.layers):\n",
        "        print(f\"Index: {i}, Layer Name: {layer.name}\")\n",
        "\n",
        "print_layers_with_index(base_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gx46vdbI7NQv",
        "outputId": "625a0b11-2eb7-4f8f-a714-c7057a48730e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-11-06 14:21:04.280146: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8900\n",
            "2023-11-06 14:21:05.336083: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
            "2023-11-06 14:21:07.576796: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5650a79aab80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2023-11-06 14:21:07.576856: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4050 Laptop GPU, Compute Capability 8.9\n",
            "2023-11-06 14:21:07.620943: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\r  1/200 [..............................] - ETA: 42:17 - loss: 1.3770 - accuracy: 0.3125"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-11-06 14:21:07.865198: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "200/200 [==============================] - 72s 298ms/step - loss: 1.1779 - accuracy: 0.4806 - val_loss: 1.1250 - val_accuracy: 0.5113\n",
            "Epoch 2/50\n",
            "200/200 [==============================] - 56s 278ms/step - loss: 1.0227 - accuracy: 0.5809 - val_loss: 1.0611 - val_accuracy: 0.5500\n",
            "Epoch 3/50\n",
            "200/200 [==============================] - 56s 281ms/step - loss: 0.9496 - accuracy: 0.6150 - val_loss: 0.9975 - val_accuracy: 0.5813\n",
            "Epoch 4/50\n",
            "200/200 [==============================] - 56s 282ms/step - loss: 0.9103 - accuracy: 0.6394 - val_loss: 0.9855 - val_accuracy: 0.5888\n",
            "Epoch 5/50\n",
            "200/200 [==============================] - 57s 284ms/step - loss: 0.8828 - accuracy: 0.6409 - val_loss: 0.9605 - val_accuracy: 0.6212\n",
            "Epoch 6/50\n",
            "200/200 [==============================] - 57s 283ms/step - loss: 0.8606 - accuracy: 0.6634 - val_loss: 0.9714 - val_accuracy: 0.5975\n",
            "Epoch 7/50\n",
            "200/200 [==============================] - 56s 281ms/step - loss: 0.8451 - accuracy: 0.6584 - val_loss: 0.9722 - val_accuracy: 0.5850\n",
            "Epoch 8/50\n",
            "200/200 [==============================] - 56s 280ms/step - loss: 0.8236 - accuracy: 0.6684 - val_loss: 0.9513 - val_accuracy: 0.6187\n",
            "Epoch 9/50\n",
            "200/200 [==============================] - 56s 279ms/step - loss: 0.8174 - accuracy: 0.6759 - val_loss: 1.0111 - val_accuracy: 0.5650\n",
            "Epoch 10/50\n",
            "200/200 [==============================] - 56s 279ms/step - loss: 0.7927 - accuracy: 0.6850 - val_loss: 0.9453 - val_accuracy: 0.6250\n",
            "Epoch 11/50\n",
            "200/200 [==============================] - 56s 281ms/step - loss: 0.7898 - accuracy: 0.6809 - val_loss: 0.9014 - val_accuracy: 0.6363\n",
            "Epoch 12/50\n",
            "200/200 [==============================] - 56s 281ms/step - loss: 0.7740 - accuracy: 0.6859 - val_loss: 0.9134 - val_accuracy: 0.6450\n",
            "Epoch 13/50\n",
            "200/200 [==============================] - 56s 278ms/step - loss: 0.7533 - accuracy: 0.6994 - val_loss: 0.9189 - val_accuracy: 0.6225\n",
            "Epoch 14/50\n",
            "200/200 [==============================] - 56s 277ms/step - loss: 0.7621 - accuracy: 0.6944 - val_loss: 0.8800 - val_accuracy: 0.6675\n",
            "Epoch 15/50\n",
            "200/200 [==============================] - 55s 277ms/step - loss: 0.7418 - accuracy: 0.7034 - val_loss: 0.9108 - val_accuracy: 0.6313\n",
            "Epoch 16/50\n",
            "200/200 [==============================] - 56s 279ms/step - loss: 0.7461 - accuracy: 0.7022 - val_loss: 0.9000 - val_accuracy: 0.6438\n",
            "Epoch 17/50\n",
            "200/200 [==============================] - 56s 280ms/step - loss: 0.7261 - accuracy: 0.7159 - val_loss: 0.9080 - val_accuracy: 0.6263\n",
            "Epoch 18/50\n",
            "200/200 [==============================] - 56s 281ms/step - loss: 0.7254 - accuracy: 0.7088 - val_loss: 0.9480 - val_accuracy: 0.6325\n",
            "Epoch 19/50\n",
            "200/200 [==============================] - 57s 282ms/step - loss: 0.7263 - accuracy: 0.7041 - val_loss: 0.8828 - val_accuracy: 0.6350\n",
            "Epoch 20/50\n",
            "200/200 [==============================] - 56s 281ms/step - loss: 0.7021 - accuracy: 0.7188 - val_loss: 0.8679 - val_accuracy: 0.6475\n",
            "Epoch 21/50\n",
            "200/200 [==============================] - 62s 309ms/step - loss: 0.7125 - accuracy: 0.7253 - val_loss: 0.8944 - val_accuracy: 0.6550\n",
            "Epoch 22/50\n",
            "200/200 [==============================] - 68s 338ms/step - loss: 0.6939 - accuracy: 0.7322 - val_loss: 0.8751 - val_accuracy: 0.6550\n",
            "Epoch 23/50\n",
            "200/200 [==============================] - 57s 286ms/step - loss: 0.6921 - accuracy: 0.7209 - val_loss: 0.9097 - val_accuracy: 0.6463\n",
            "Epoch 24/50\n",
            "200/200 [==============================] - 56s 282ms/step - loss: 0.6919 - accuracy: 0.7337 - val_loss: 0.8512 - val_accuracy: 0.6712\n",
            "Epoch 25/50\n",
            "200/200 [==============================] - 57s 284ms/step - loss: 0.6805 - accuracy: 0.7331 - val_loss: 0.8641 - val_accuracy: 0.6625\n",
            "Epoch 26/50\n",
            "200/200 [==============================] - 56s 281ms/step - loss: 0.6776 - accuracy: 0.7341 - val_loss: 0.8923 - val_accuracy: 0.6363\n",
            "Epoch 27/50\n",
            "200/200 [==============================] - 57s 285ms/step - loss: 0.6706 - accuracy: 0.7359 - val_loss: 0.8793 - val_accuracy: 0.6475\n",
            "Epoch 28/50\n",
            "200/200 [==============================] - 57s 284ms/step - loss: 0.6781 - accuracy: 0.7300 - val_loss: 0.8854 - val_accuracy: 0.6488\n",
            "Epoch 29/50\n",
            "200/200 [==============================] - 57s 285ms/step - loss: 0.6510 - accuracy: 0.7541 - val_loss: 0.8745 - val_accuracy: 0.6637\n",
            "Epoch 30/50\n",
            "200/200 [==============================] - 57s 283ms/step - loss: 0.6595 - accuracy: 0.7375 - val_loss: 0.8709 - val_accuracy: 0.6775\n",
            "Epoch 31/50\n",
            "200/200 [==============================] - 56s 280ms/step - loss: 0.6582 - accuracy: 0.7431 - val_loss: 0.9167 - val_accuracy: 0.6375\n",
            "Epoch 32/50\n",
            "200/200 [==============================] - 56s 281ms/step - loss: 0.6523 - accuracy: 0.7484 - val_loss: 0.8312 - val_accuracy: 0.6625\n",
            "Epoch 33/50\n",
            "200/200 [==============================] - 57s 283ms/step - loss: 0.6569 - accuracy: 0.7472 - val_loss: 0.8721 - val_accuracy: 0.6600\n",
            "Epoch 34/50\n",
            "200/200 [==============================] - 56s 281ms/step - loss: 0.6509 - accuracy: 0.7525 - val_loss: 0.8726 - val_accuracy: 0.6612\n",
            "Epoch 35/50\n",
            "200/200 [==============================] - 56s 281ms/step - loss: 0.6498 - accuracy: 0.7469 - val_loss: 0.8750 - val_accuracy: 0.6562\n",
            "Epoch 36/50\n",
            "200/200 [==============================] - 56s 281ms/step - loss: 0.6376 - accuracy: 0.7484 - val_loss: 0.8667 - val_accuracy: 0.6625\n",
            "Epoch 37/50\n",
            "200/200 [==============================] - 56s 281ms/step - loss: 0.6364 - accuracy: 0.7594 - val_loss: 0.8530 - val_accuracy: 0.6612\n",
            "Epoch 38/50\n",
            "200/200 [==============================] - 56s 280ms/step - loss: 0.6254 - accuracy: 0.7581 - val_loss: 0.8338 - val_accuracy: 0.6737\n",
            "Epoch 39/50\n",
            "200/200 [==============================] - 65s 325ms/step - loss: 0.6276 - accuracy: 0.7619 - val_loss: 0.8293 - val_accuracy: 0.6612\n",
            "Epoch 40/50\n",
            "200/200 [==============================] - 60s 299ms/step - loss: 0.6173 - accuracy: 0.7559 - val_loss: 0.8417 - val_accuracy: 0.6850\n",
            "Epoch 41/50\n",
            "200/200 [==============================] - 46s 230ms/step - loss: 0.6247 - accuracy: 0.7569 - val_loss: 0.8297 - val_accuracy: 0.6625\n",
            "Epoch 42/50\n",
            "200/200 [==============================] - 47s 236ms/step - loss: 0.6252 - accuracy: 0.7506 - val_loss: 0.8393 - val_accuracy: 0.6712\n",
            "Epoch 43/50\n",
            "200/200 [==============================] - 47s 232ms/step - loss: 0.6043 - accuracy: 0.7647 - val_loss: 0.8212 - val_accuracy: 0.6712\n",
            "Epoch 44/50\n",
            "200/200 [==============================] - 47s 237ms/step - loss: 0.6093 - accuracy: 0.7613 - val_loss: 0.8447 - val_accuracy: 0.6600\n",
            "Epoch 45/50\n",
            "200/200 [==============================] - 47s 235ms/step - loss: 0.6237 - accuracy: 0.7616 - val_loss: 0.8442 - val_accuracy: 0.6600\n",
            "Epoch 46/50\n",
            "200/200 [==============================] - 46s 231ms/step - loss: 0.6137 - accuracy: 0.7628 - val_loss: 0.8542 - val_accuracy: 0.6712\n",
            "Epoch 47/50\n",
            "200/200 [==============================] - 46s 228ms/step - loss: 0.6120 - accuracy: 0.7600 - val_loss: 0.8917 - val_accuracy: 0.6488\n",
            "Epoch 48/50\n",
            "200/200 [==============================] - 46s 229ms/step - loss: 0.6162 - accuracy: 0.7625 - val_loss: 0.8709 - val_accuracy: 0.6575\n",
            "Epoch 49/50\n",
            "200/200 [==============================] - 45s 226ms/step - loss: 0.6018 - accuracy: 0.7641 - val_loss: 0.8225 - val_accuracy: 0.6875\n",
            "Epoch 50/50\n",
            "200/200 [==============================] - 49s 243ms/step - loss: 0.5943 - accuracy: 0.7678 - val_loss: 0.8536 - val_accuracy: 0.6525\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-11-06 15:07:53.035855: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 144.00MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
            "2023-11-06 15:07:53.362601: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 144.00MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
            "2023-11-06 15:07:53.453880: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 490.41MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
            "2023-11-06 15:07:53.634143: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 42.88MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
            "2023-11-06 15:07:53.638189: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 55.63MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
            "2023-11-06 15:07:53.640412: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 57.13MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
            "2023-11-06 15:07:53.647590: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 42.88MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
            "2023-11-06 15:07:53.653846: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 42.31MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
            "2023-11-06 15:07:53.659692: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 43.63MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
            "2023-11-06 15:07:53.659736: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 42.88MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "200/200 [==============================] - 143s 398ms/step - loss: 0.6735 - accuracy: 0.7325 - val_loss: 0.7401 - val_accuracy: 0.7100\n",
            "Epoch 2/50\n",
            "200/200 [==============================] - 78s 388ms/step - loss: 0.4600 - accuracy: 0.8275 - val_loss: 0.7700 - val_accuracy: 0.7312\n",
            "Epoch 3/50\n",
            "200/200 [==============================] - 78s 389ms/step - loss: 0.3686 - accuracy: 0.8647 - val_loss: 0.7108 - val_accuracy: 0.7912\n",
            "Epoch 4/50\n",
            "200/200 [==============================] - 78s 388ms/step - loss: 0.2925 - accuracy: 0.8944 - val_loss: 0.5971 - val_accuracy: 0.8025\n",
            "Epoch 5/50\n",
            "200/200 [==============================] - 78s 388ms/step - loss: 0.2316 - accuracy: 0.9194 - val_loss: 0.5331 - val_accuracy: 0.8250\n",
            "Epoch 6/50\n",
            "200/200 [==============================] - 78s 388ms/step - loss: 0.2001 - accuracy: 0.9319 - val_loss: 0.6016 - val_accuracy: 0.8138\n",
            "Epoch 7/50\n",
            "200/200 [==============================] - 78s 388ms/step - loss: 0.1627 - accuracy: 0.9478 - val_loss: 0.6372 - val_accuracy: 0.8225\n",
            "Epoch 8/50\n",
            "200/200 [==============================] - 77s 387ms/step - loss: 0.1403 - accuracy: 0.9556 - val_loss: 0.6701 - val_accuracy: 0.8263\n",
            "Epoch 9/50\n",
            "200/200 [==============================] - 78s 387ms/step - loss: 0.1302 - accuracy: 0.9591 - val_loss: 0.6377 - val_accuracy: 0.8313\n",
            "Epoch 10/50\n",
            "200/200 [==============================] - 78s 388ms/step - loss: 0.1060 - accuracy: 0.9659 - val_loss: 0.6332 - val_accuracy: 0.8238\n",
            "Epoch 11/50\n",
            "200/200 [==============================] - 78s 388ms/step - loss: 0.0885 - accuracy: 0.9734 - val_loss: 0.5652 - val_accuracy: 0.8388\n",
            "Epoch 12/50\n",
            "200/200 [==============================] - 78s 388ms/step - loss: 0.0761 - accuracy: 0.9784 - val_loss: 0.6482 - val_accuracy: 0.8325\n",
            "Epoch 13/50\n",
            "200/200 [==============================] - 78s 389ms/step - loss: 0.0746 - accuracy: 0.9766 - val_loss: 0.6223 - val_accuracy: 0.8313\n",
            "Epoch 14/50\n",
            "200/200 [==============================] - 78s 388ms/step - loss: 0.0517 - accuracy: 0.9844 - val_loss: 0.7322 - val_accuracy: 0.8350\n",
            "Epoch 15/50\n",
            "200/200 [==============================] - 78s 388ms/step - loss: 0.0534 - accuracy: 0.9869 - val_loss: 0.6621 - val_accuracy: 0.8375\n",
            "Epoch 16/50\n",
            "200/200 [==============================] - 78s 387ms/step - loss: 0.0450 - accuracy: 0.9872 - val_loss: 0.8454 - val_accuracy: 0.8363\n",
            "Epoch 17/50\n",
            "200/200 [==============================] - 78s 387ms/step - loss: 0.0389 - accuracy: 0.9903 - val_loss: 0.6537 - val_accuracy: 0.8375\n",
            "Epoch 18/50\n",
            "200/200 [==============================] - 78s 387ms/step - loss: 0.0389 - accuracy: 0.9906 - val_loss: 0.8014 - val_accuracy: 0.8325\n",
            "Epoch 19/50\n",
            "200/200 [==============================] - 78s 388ms/step - loss: 0.0416 - accuracy: 0.9881 - val_loss: 0.6541 - val_accuracy: 0.8400\n",
            "Epoch 20/50\n",
            "200/200 [==============================] - 78s 387ms/step - loss: 0.0368 - accuracy: 0.9875 - val_loss: 0.7510 - val_accuracy: 0.8375\n",
            "Epoch 21/50\n",
            "200/200 [==============================] - 78s 387ms/step - loss: 0.0336 - accuracy: 0.9900 - val_loss: 0.8169 - val_accuracy: 0.8275\n",
            "Epoch 22/50\n",
            "200/200 [==============================] - 78s 388ms/step - loss: 0.0286 - accuracy: 0.9903 - val_loss: 0.6651 - val_accuracy: 0.8525\n",
            "Epoch 23/50\n",
            "200/200 [==============================] - 78s 387ms/step - loss: 0.0283 - accuracy: 0.9909 - val_loss: 0.6518 - val_accuracy: 0.8438\n",
            "Epoch 24/50\n",
            "200/200 [==============================] - 78s 388ms/step - loss: 0.0252 - accuracy: 0.9934 - val_loss: 0.7014 - val_accuracy: 0.8462\n",
            "Epoch 25/50\n",
            "200/200 [==============================] - 78s 388ms/step - loss: 0.0249 - accuracy: 0.9931 - val_loss: 0.7229 - val_accuracy: 0.8475\n",
            "Epoch 26/50\n",
            "200/200 [==============================] - 78s 387ms/step - loss: 0.0236 - accuracy: 0.9937 - val_loss: 0.7400 - val_accuracy: 0.8587\n",
            "Epoch 27/50\n",
            "200/200 [==============================] - 78s 388ms/step - loss: 0.0225 - accuracy: 0.9931 - val_loss: 0.7322 - val_accuracy: 0.8625\n",
            "Epoch 28/50\n",
            "200/200 [==============================] - 78s 388ms/step - loss: 0.0235 - accuracy: 0.9934 - val_loss: 0.9296 - val_accuracy: 0.8462\n",
            "Epoch 29/50\n",
            "200/200 [==============================] - 78s 388ms/step - loss: 0.0231 - accuracy: 0.9934 - val_loss: 0.7713 - val_accuracy: 0.8725\n",
            "Epoch 30/50\n",
            "200/200 [==============================] - 78s 388ms/step - loss: 0.0224 - accuracy: 0.9909 - val_loss: 0.7212 - val_accuracy: 0.8525\n",
            "Epoch 31/50\n",
            "200/200 [==============================] - 78s 388ms/step - loss: 0.0274 - accuracy: 0.9919 - val_loss: 0.6261 - val_accuracy: 0.8475\n",
            "Epoch 32/50\n",
            "200/200 [==============================] - 78s 388ms/step - loss: 0.0165 - accuracy: 0.9959 - val_loss: 0.7088 - val_accuracy: 0.8625\n",
            "Epoch 33/50\n",
            "200/200 [==============================] - 78s 388ms/step - loss: 0.0198 - accuracy: 0.9944 - val_loss: 0.6704 - val_accuracy: 0.8650\n",
            "Epoch 34/50\n",
            "200/200 [==============================] - 78s 388ms/step - loss: 0.0229 - accuracy: 0.9909 - val_loss: 0.8046 - val_accuracy: 0.8462\n",
            "Epoch 35/50\n",
            "200/200 [==============================] - 78s 388ms/step - loss: 0.0121 - accuracy: 0.9972 - val_loss: 0.8632 - val_accuracy: 0.8537\n",
            "Epoch 36/50\n",
            "200/200 [==============================] - 78s 387ms/step - loss: 0.0150 - accuracy: 0.9966 - val_loss: 0.7343 - val_accuracy: 0.8687\n",
            "Epoch 37/50\n",
            "200/200 [==============================] - 78s 388ms/step - loss: 0.0157 - accuracy: 0.9950 - val_loss: 0.9675 - val_accuracy: 0.8400\n",
            "Epoch 38/50\n",
            "200/200 [==============================] - 78s 388ms/step - loss: 0.0162 - accuracy: 0.9944 - val_loss: 0.7274 - val_accuracy: 0.8625\n",
            "Epoch 39/50\n",
            "200/200 [==============================] - 78s 387ms/step - loss: 0.0114 - accuracy: 0.9966 - val_loss: 0.8475 - val_accuracy: 0.8587\n",
            "Epoch 40/50\n",
            "200/200 [==============================] - 78s 387ms/step - loss: 0.0141 - accuracy: 0.9950 - val_loss: 0.9869 - val_accuracy: 0.8637\n",
            "Epoch 41/50\n",
            "200/200 [==============================] - 78s 388ms/step - loss: 0.0087 - accuracy: 0.9972 - val_loss: 1.3509 - val_accuracy: 0.8562\n",
            "Epoch 42/50\n",
            "200/200 [==============================] - 78s 387ms/step - loss: 0.0127 - accuracy: 0.9962 - val_loss: 0.9924 - val_accuracy: 0.8587\n",
            "Epoch 43/50\n",
            "200/200 [==============================] - 78s 391ms/step - loss: 0.0122 - accuracy: 0.9959 - val_loss: 0.9720 - val_accuracy: 0.8625\n",
            "Epoch 44/50\n",
            "200/200 [==============================] - 78s 387ms/step - loss: 0.0132 - accuracy: 0.9969 - val_loss: 1.1279 - val_accuracy: 0.8512\n",
            "Epoch 45/50\n",
            "200/200 [==============================] - 77s 386ms/step - loss: 0.0154 - accuracy: 0.9950 - val_loss: 0.9253 - val_accuracy: 0.8587\n",
            "Epoch 46/50\n",
            "200/200 [==============================] - 78s 388ms/step - loss: 0.0150 - accuracy: 0.9953 - val_loss: 1.1233 - val_accuracy: 0.8537\n",
            "Epoch 47/50\n",
            "200/200 [==============================] - 78s 388ms/step - loss: 0.0101 - accuracy: 0.9978 - val_loss: 0.8412 - val_accuracy: 0.8575\n",
            "Epoch 48/50\n",
            "200/200 [==============================] - 77s 386ms/step - loss: 0.0144 - accuracy: 0.9959 - val_loss: 0.8565 - val_accuracy: 0.8737\n",
            "Epoch 49/50\n",
            "200/200 [==============================] - 77s 386ms/step - loss: 0.0156 - accuracy: 0.9944 - val_loss: 0.9651 - val_accuracy: 0.8712\n",
            "Epoch 50/50\n",
            "200/200 [==============================] - 78s 387ms/step - loss: 0.0123 - accuracy: 0.9966 - val_loss: 0.8912 - val_accuracy: 0.8637\n",
            "50/50 [==============================] - 9s 183ms/step - loss: 0.7868 - accuracy: 0.8637\n",
            "Validation Loss: 0.7868\n",
            "Validation Accuracy: 0.8637\n"
          ]
        }
      ],
      "source": [
        "# InceptionResNetV2 모델 층들을 동결\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# 컴파일\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 모델 학습\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
        "    epochs=50\n",
        ")\n",
        "\n",
        "# InceptionResNetV2의 모델 층들을 동결 해제\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = True\n",
        "\n",
        "# 컴파일\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(1e-5),\n",
        "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 모델 학습\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
        "    epochs=50)\n",
        "\n",
        "# 모델 평가\n",
        "loss, accuracy = model.evaluate(validation_generator)\n",
        "print(f\"Validation Loss: {loss:.4f}\")\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
